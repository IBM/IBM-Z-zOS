<?xml version="1.0" encoding="UTF-8"?>
<!--  
/*********************************************************************/
/* Copyright 2017 IBM Corp.                                          */
/*                                                                   */
/* Licensed under the Apache License, Version 2.0 (the "License");   */
/* you may not use this file except in compliance with the License.  */
/* You may obtain a copy of the License at                           */
/*                                                                   */
/* http://www.apache.org/licenses/LICENSE-2.0                        */
/*                                                                   */
/* Unless required by applicable law or agreed to in writing, software*/
/* distributed under the License is distributed on an "AS IS" BASIS, */
/* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. */
/* See the License for the specific language governing permissions and */
/* limitations under the License.                                   */
/*********************************************************************/     
-->
<workflow xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="workflow_v1.xsd">                          
		  
	<workflowInfo>
	<workflowID scope="system" isCallable="system">sparkci</workflowID><workflowDescription>Install and Configure IBM Platform for Apache Spark Software</workflowDescription>
		<workflowVersion>1</workflowVersion>
		<vendor>IBM</vendor>	</workflowInfo>

<step name="Step1"><title>Ensure IBM Platform for Apache Spark Software requirements are met.</title><description>Ensure you have the software requirements identified by all the steps in this task.  
For the latest list of software requirements, see 
IBM z/OS Platform for Apache Spark Product Details at: <a href="http://www-03.ibm.com/systems/z/os/zos/apache-spark.html">http://www-03.ibm.com/systems/z/os/zos/apache-spark.html</a>. </description><step name="subStep1_1"><title>Verify that IBM z/OS V2.1, or later, is installed.</title><description>Verify that IBM z/OS V2.1, or later, is installed.</description><instructions>Verify that IBM z/OS V2.1, or later, is installed.</instructions><weight>1</weight></step><step name="subStep1_2"><title>Verify that IBM 64-Bit SDK for z/OS, Java Technology Edition at the correct level is installed</title><description>The minimum Java level required for z/OS Platform for Apache Spark is IBM 64-Bit SDK for z/OS, Java Technology Edition, Fix Version 8 Refresh 3 Fix Pack 12. 
However, if the RELEASE file in SPARK_HOME indicates the product was built with a higher Java level, 
IBM suggests you use that higher Java level. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description><instructions>
The default path to IBM 64-Bit SDK for z/OS Java Technology Edition V8 is
/usr/lpp/java/J8.0_64. If your path is different, make note of it as it will be used for later customization.</instructions><weight>1</weight></step><step name="Step1_3"><title>Verify that bash (Bourne Again Shell) version 4.2.53 is installed.</title><description>Verify that bash (Bourne Again Shell) version 4.2.53 is installed </description><step name="subStep1_3_1">				<title>Check to see whether and what version of bash is installed.</title><description>Check to see whether and what version of bash is installed.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>To see whether and what version of bash is installed, issue the following command from z/OS UNIX (OpenSSH or OMVS):
<p><code>bash -version</code></p></instructions><weight>1</weight>	
</step><step name="subStep1_3_2">				<title>If bash is not found, search for it.</title><description>If you don't find bash after issuing the command in the previous step, bash might be installed but not in your PATH. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description><instructions><ul><li>Search your system for bash. The default path to bash 4.2.x is /usr/bin/bash-4.2, though your system might contain more than one instance of bash.<p><b>Tip:</b> Use the <code>find</code> command to search for bash.  For example, the following command searches for all files named "bash" from the root ("/") directory:</p><p><code>find / -name "bash"</code></p><p><b>Note: </b>This command traverses your whole system, and may take some time. You will see access errors if you issue this command from a user ID without sufficient authority to traverse and access all the directories/files in your system.
In this case, consider narrowing the search to directories which are likely candidates for product installs (for example,  /usr/bin or /usr/lpp).</p></li><li>Go to all the directories where you found instances of bash and run <code>./bash -version</code> to call the instance and check the version.</li></ul></instructions><weight>1</weight>	
</step><step name="subStep1_3_3">				<title>If the required version of bash is not installed, download and install the latest level of bash</title><description>If bash is not installed or the required version is not installed, download and install the 4.2.53 level of bash from Rocket z/OS Open Source Community Downloads 
(<a href="http://www.rocketsoftware.com/ported-tools">www.rocketsoftware.com/ported-tools</a>).<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Select to download bash and follow the instructions to register with Rocket. You can then download the archive file and the "Getting Started" document.<p>Tips: The bash installation process involves the following actions:<ol><li>Create a mount point and file system on z/OS to hold the bash files</li><li>Upload in binary the archive file into your new file system
<li>Extract the archive file with command <code>gzip –d filename.tar.gz</code><li>Extract the file with command <code>tar –xvfo filename.tar</code></li></li>
</li></ol></p></instructions><weight>1</weight>	
</step><step name="subStep1_3_4">				<title>Make a note of the path to the bash /bin directory</title><description>Directories /bin and /man represent the bash shell code.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions> Make note of the path to the bash /bin directory for configuration later.</instructions><weight>1</weight>	
</step></step><step name="subStep1_4"><title>Verify the install location for the env command</title><description>Apache Spark is dependent on the env command being located in /usr/bin.</description><step name="subStep1_4_1">				<title>Test the path for env</title><description>Test the path for env, in an SSH or Telnet environment. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>To test the path for env, in an SSH or Telnet environment run the following command to verify the location and contents of env: 
<p><code>/usr/bin/env</code></p>
The command should return a list of name and value pairs for the environment in your shell. <p>If /usr/bin/env does not exist, complete the following steps to set it up:<ol>
<li>Locate the env program on your system.  A potential location is in /bin/env.</li>
<li>Create a symbolic link (symlink) so that /usr/bin/env resolves to the true location of env.  For example:
<p><code>ln -s /bin/env /usr/bin/env</code></p></li>
<li>Optionally, in an SSH or Telnet shell environment, run the following command to verify that the symlink works:
<p><code>/usr/bin/env</code></p></li></ol>
The command should return a list of name and value pairs for the environment in your shell.
</p></instructions><weight>1</weight>	
</step><step name="subStep1_4_2">				<title>Ensure the symbolic link for the env command persists across IPLs</title><description>Ensure the symbolic link for the env command persists across IPLs.
<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Depending on how /usr/bin/ is configured on your system, the symbolic link created for /usr/bin/env may not persist across an IPL without additional setup.  
Please ensure your IPL setup includes creation of this symbolic link if needed.
</instructions><weight>1</weight>	
</step></step></step><step name="Step2"><title>Ensure that IBM z/OS UNIX configuration requirements are met</title><description>Ensure that IBM z/OS UNIX configuration requirements are met</description><step name="subStep2_1"><title>Verify your z/OS UNIX environment is correctly configured</title><description>Spark runs in z/OS UNIX.  If this is the first time you are running applications in z/OS UNIX, see 
<a href="https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.1.0/com.ibm.zos.v2r1.bpxb200/toc.htm">z/OS UNIX System Services Planning</a> to ensure that your z/OS UNIX environment is properly configured and customized.
<p>See the <b>Perform</b> workflow tab for further instructions.</p></description><instructions>An example of configuring z/OS UNIX correctly for Spark is that Spark should not be run as UID 0.  However, if you choose to run Spark as UID 0, 
in an environment where multiple users are mapped to UID 0, you may encounter problems with the wrong shell profile being read and the required environment variables not being set.   
<p>For example, $HOME/.profile might be read for user BOB mapped to UID 0, when you really wanted the shell profile for SPARKID (also mapped to UID 0) to be read. </p><p>
See <cite>Superusers in z/OS UNIX</cite> 
(<a href="https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.1.0/com.ibm.zos.v2r1.bpxb200/seca.htm">https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.1.0/com.ibm.zos.v2r1.bpxb200/seca.htm</a>) in 
 <a href="https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.1.0/com.ibm.zos.v2r1.bpxb200/toc.htm">z/OS UNIX System Services Planning</a> 
for alternatives to setting multiple user IDs as UID 0.
</p></instructions><weight>1</weight></step><step name="subStep2_2"><title>Ensure that your z/OS UNIX environment has sufficient memory configured </title><description>Ensure that your z/OS UNIX environment has sufficient memory configured for extended common service area (ECSA) and extended system queue area (ESQA).  <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>See  <cite>Evaluating virtual memory needs</cite> 
(<a href="https://www.ibm.com/support/knowledgecenter/SSLTBW_2.2.0/com.ibm.zos.v2r2.bpxb200/scmvs.htm">https://www.ibm.com/support/knowledgecenter/SSLTBW_2.2.0/com.ibm.zos.v2r2.bpxb200/scmvs.htm</a>) in 
<cite>z/OS UNIX System Services Planning</cite>.</instructions><weight>1</weight></step><step name="Step2_3"><title>Optionally enable IBM Health Checker for z/OS checks for z/OS Unix and other z/OS system services</title><description>Optionally consider enabling IBM Health Checker for z/OS checks for z/OS UNIX and other z/OS system services.  For more information, see 
<cite>IBM Health Checker for z/OS User's Guide</cite>.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>The following IBM Health Checker for z/OS checks might be useful for Spark:
<ul>
<li>RACF_UNIX_ID</li>
<li>RSM_MEMLIMIT</li>
<li>RSM_AFQ</li>
<li>IEA_ASIDS</li>
<li>USS_AUTOMOUNT_DELAY</li>
<li>USS_FILESYS_CONFIG</li>
<li>USS_MAXSOCKETS_MAXFILEPROC</li>
<li>USS_CLIENT_MOUNTS</li>
<li>USS_KERNEL_PVTSTG_THRESHOLD</li>
<li>USS_KERNEL_STACKS_THRESHOLD</li>
<li>VSM_CSA_THRESHOLD</li>
<li>VSM_SQA_THRESHOLD</li></ul></instructions><weight>1</weight></step></step><step name="Step3"><title>Install and customize IBM z/OS Platform for Apache Spark  </title><description>This task enumerates the installation and customization steps from <cite>IBM z/OS Platform for Apache Spark Installation and Customization Guide</cite>.</description><step name="Step3_1"><title>Install IBM z/OS Platform for Apache Spark on your system</title><description>Install IBM z/OS Platform for Apache Spark on your system as detailed in <cite>Installing IBM z/OS Platform for Apache Spark</cite> (<code>https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azkic_t_install.htm</code>) in
<cite>IBM z/OS Platform for Apache Spark Installation and Customization Guide</cite>.</description><step name="Step3_1_1"><title>Choose the most appropriate method for installing IBM z/OS Platform for Apache Spark</title><description>Choose the most appropriate method for installing IBM z/OS Platform for Apache Spark.</description>
<instructions>Choose the most appropriate method for installing IBM z/OS Platform for Apache Spark.</instructions><weight>1</weight></step><step name="Step3_1_2"><title>Installing IBM z/OS Platform for Apache Spark on your system</title><description>Use the information in the Program Directory for IBM z/OS Platform for Apache Spark, the PSP bucket and, if applicable, the PTF cover letter to install IBM z/OS Platform for Apache Spark on your system.</description>
<instructions>Use the information in the Program Directory for IBM z/OS Platform for Apache Spark, the PSP bucket and, if applicable, the PTF cover letter to install IBM z/OS Platform for Apache 
Spark on your system.</instructions><weight>1</weight></step></step><step name="Step3_2"><title>Set up the user ID for use with IBM z/OS Platform for Apache Spark</title><description>Set up the user ID for use with IBM z/OS Platform for Apache Spark.  Specifically, this is the user ID under which the Spark cluster is started, and will be referred to as the SPARK ID.
Work with your security administrator to configure the SPARK ID. The SPARK ID should be UID non-0 (not a superuser), and should not have additional access to any data 
beyond what it needs for running a Spark cluster. </description>
<step name="Step3_2_1"><title>Ensure that the SPARK ID's OMVS segment's default shell program is set to the bash shell. </title><description>Ensure that the SPARK ID's OMVS segment's default shell program is set to the bash shell. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Do one of the following:<ul><li><b>Using an existing user ID for the SPARK ID</b><p>If you are using an existing user ID for the SPARK ID, determine if the PROGRAM attribute of the OMVS segment is valid for the SPARK ID:
<ol><li>Use SSH to log on using the SPARK ID.</li>
<li>Run echo $SHELL and review the output.</li></ol><p>If bash is still not listed as the default shell for the SPARK ID, a potential reason is because /etc/profile is providing an explicit invocation of the shell 
other than bash.  If so, work with your system administrator to update /etc/profile to define the operative shell in the OMVS segment.  
The following code provides an example of how one might override the shell set by the OMVS segment:</p><p><code>if [ -z "$STEPLIB" ] &amp;&amp; tty -s;
then
export STEPLIB=none
exec -a $0 $SHELL -
fi
</code></p></p></li><li><b>Creating a new user ID for the SPARK ID</b><p>If creating a new user ID for the SPARK ID, establish the OMVS segment during creation.  The following JCL example shows how to create a new user ID and group for the 
SPARK ID "SPARKID", which will be used run an IBM z/OS Platform for Apache Spark cluster:
<p><code>//SPARK JOB (0),’SPARK RACF’,CLASS=A,REGION=0M,
//         	MSGCLASS=H,NOTIFY=&amp;SYSUID
//*------------------------------------------------------------*/
//RACF       	EXEC PGM=IKJEFT01,REGION=0M
//SYSTSPRT 	DD SYSOUT=*
//SYSTSIN  	DD *
ADDGROUP SPKGRP OMVS(AUTOGID) OWNER(SYS1)
ADDUSER SPARKID DFLTGRP(SPKGRP) OMVS(AUTOUID HOME(/u/sparkid) -
PROGRAM(/shared/rocket/bash-4.2/bin/bash)) -
NAME(’Spark ID’) NOPASSWORD NOOIDCARD
ALTUSER SPARKID PASSWORD(SPARKID) NOEXPIRED
/*
</code></p><p><b>Notes:</b></p><p><ul>
<li>Use of AUTOGID and AUTOUID in the example is based on a local preference.  	Your coding might differ.</li>
<li>Set the PROGRAM attribute to define the path to your own installation of bash 4.2.53 as noted in step 1.3.4.</li>
</ul></p></p></li></ul></instructions><weight>1</weight></step></step><step name="Step3_3"><title>Configure the z/OS UNIX shell environment for SPARK ID and users</title><description>Configure the z/OS UNIX shell environment for both your SPARK ID and all users of Spark.  z/OS Platform for Apache Spark requires certain environment variables to be set. </description>
<step name="Step3_3_1"><title>Consider the scope under which you want this environment to take effect</title><description>Consider the scope under which you want this environment to take effect. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description><instructions>Consider the following: 
<ul><li>Do you want to configure Spark for all users or a subset of users?</li>
<li>Do you have other java applications that require a different level of java or require different (conflicting) java settings?</li></ul>
At a high level, this environment can be set for all users of both shells, an individual user's shell environment, or, for some settings, only for users only when they issue Spark commands.
<p>Minimally, the environment is required to be set up for the SPARK ID, and each user of Spark.</p>
<p>Use the table below to decide where to set each environment variable.  Note this table applies for users with either a login shell of bash or /bin/sh.</p>
<p><table frame="box"><tr><th>Set the environment variable here:</th><th>If you want it to have the following scope:</th></tr><tr><td>/etc/profile</td><td>All users, all the time</td></tr>
<tr><td>$HOME/.profile for a specific user</td><td>Specific users all the time</td></tr>
<tr><td>spark-env.sh</td><td>Specific users only for Spark commands</td></tr></table></p>
<p><ul><li>Values set in the $HOME/.profile file override the values set in the /etc/profile system file.  </li>
<li>Values set in spark-env.sh override any values set previously in either /etc/profile or $HOME/.profile.</li> </ul></p>
Take note of which files you want to update for the next step in the workflow.  
Creation and customization of spark-env.sh will be discussed in a later step.<p><b>Note:</b> If the SPARK ID does not already have a $HOME/.profile file, create one.
</p></instructions><weight>1</weight></step><step name="Step3_3_2"><title>Edit the files identified in step 3.3.1 and set the environment variables</title><description>For the files identified in step 3.3.1, edit each to set the environment variables.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description><instructions>For the files identified in step 3.3.1, edit each to set the environment variables as follows:
<p><ol><li>Set JAVA_HOME to point to the location of IBM 64-bit SDK for z/OS Java Technology Edition V8 (as noted in step 1.2).</li>
<li>Set PATH to include the /bin directory of IBM 64-bit SDK for z/OS Java Technology Edition V8. <p><b>Tip:</b> You can set this value by using $JAVA_HOME.</p></li><li>Set  PATH to prioritize the path to the /bin directory of bash 4.2.53 higher than any earlier version of bash that exists on your system.  <p>
<b>NOTE:</b> You must set PATH in either /etc/profile or a user's $HOME/.profile, not in spark-env.sh.</p></li><li>Set IBM_JAVA_OPTIONS to assign file encoding to ISO8859-1.</li>
<li>Set _BPXK_AUTOCVT to ON to enable the automatic conversion of tagged files.</li>
<li>Include an export statement to make all the variables available to the z/OS UNIX shell environment. </li></ol></p>
<p>The following example illustrates how to code a .profile file for these environment variable settings:</p>
<p><code># SPARK ID .profile
JAVA_HOME=/shared/java/java_1.8_64
PATH=$JAVA_HOME/bin:/shared/rocket/bash-4.2/bin:$PATH
IBM_JAVA_OPTIONS="-Dfile.encoding=ISO8859-1"
_BPXK_AUTOCVT=ON

#This line sets the prompt
PS1=’$LOGNAME’:’$PWD’:’>’

#This line exports the variable settings
export JAVA_HOME PATH IBM_JAVA_OPTIONS _BPXK_AUTOCVT PS1
</code></p>
<b>Note:</b> The same syntax applies for /etc/profile, $HOME/.profile and spark-env.sh. 
</instructions><weight>1</weight></step><step name="Step3_3_3"><title>Optionally verify information is set properly for the SPARK environment</title><description>Optionally verify the bash version, bash default, JAVA_HOME, file encoding and automatic conversion of tagged files are set 
properly for the Spark environment under SPARK ID.</description><step name="Step3_3_3_1"><title>Open a new login shell for the SPARK ID</title><description>Open a new login shell for the SPARK ID, using OMVS or SSH.</description>
<instructions>Open a new login shell for the SPARK ID, using OMVS or SSH.</instructions><weight>1</weight></step><step name="Step3_3_3_2"><title>Verify the bash version</title><description>Verify that the bash version is set to 4.2.53. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>To verify the bash version is set to 4.2.53 issue the command:
<p><code>bash -version</code></p>
<p>If the version returned is incorrect, check the PATH value in the $HOME/.profile or /etc/profile file lists the latest version of the bash /bin directory before any other bash installations.
</p></instructions><weight>1</weight></step><step name="Step3_3_3_3"><title>Verify the bash is the default shell </title><description>Verify the bash is the default shell for the currently logged in SPARK ID.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>To verify the bash is the default shell for the currently logged in SPARK ID, issue the command:
<p><code>ps -p $$</code></p>
<p>The command returns the value of the process ID and indicates the shell program that is used, for example:</p>
<p><code># ps -p $$                                           
       PID TTY       TIME CMD                        
  33619981 ttyp0000  0:00 /usr/bin/bash-4.2/bin/bash
</code></p>
<p>If the latest copy of bash is not listed, something in /etc/profile may be  overriding the shell.  Ensure that /etc/profile is correct.</p></instructions>
<weight>1</weight></step><step name="Step3_3_3_4"><title>Verify that JAVA_HOME is set to IBM 64-bit SDK for z/OS Java Technology Edition V8</title><description>Verify that JAVA_HOME is set to IBM 64-bit SDK for z/OS Java Technology Edition V8.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>To verify that JAVA_HOME is set to IBM 64-bit SDK for z/OS Java Technology Edition V8 issue the command:
<p><code>java -version</code></p>
<p>You should see output similar to:</p><p><code>java version "1.8.0"
Java(TM) SE Runtime Environment (build pmz6480sr3fp22-20161213_02(SR3 FP22))
IBM J9 VM (build 2.8, JRE 1.8.0 z/OS s390x-64 Compressed References 20161209_329148 (JIT enabled, AOT enabled)
J9VM - R28_20161209_1345_B329148
JIT  - tr.r14.java.green_20161207_128946
GC   - R28_20161209_1345_B329148_CMPRSS
J9CL - 20161209_329148)
JCL - 20161213_01 based on Oracle jdk8u111-b14</code></p>
<p>If that output is not correct, or java is not found, issue:</p>
<p><code>echo $JAVA_HOME</code></p>
<p>The command should return the path to IBM 64-bit SDK for z/OS Java Technology Edition V8. </p>
<p>If not, ensure that the JAVA_HOME value is set correctly in the /etc/profile or $HOME/.profile file.</p></instructions>
<weight>1</weight></step><step name="Step3_3_3_5"><title>Verify the correct file encoding </title><description>Verify the correct file encoding. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>To verify the correct file encoding issue the command:
<p><code>echo $IBM_JAVA_OPTIONS</code></p>

The command output should include "<code>-Dfile.encoding=ISO8859-1</code>".  If it does not, ensure that the IBM_JAVA_OPTIONS value is set correctly in the .$HOME/profile file.
</instructions>
<weight>1</weight></step><step name="Step3_3_3_6"><title>Verify automatic conversion of tagged files  </title><description>Verify automatic conversion of tagged files. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>To verify the automatic conversion of tagged files issue the command:
<p><code>echo $_BPXK_AUTOCVT</code></p>

<p>The command returns ON.  If not, ensure that the _BPXK_AUTOCVT value is set correctly in the $HOME/.profile file.</p></instructions>
<weight>1</weight></step></step></step><step name="subStep3_4"><title>Customize the directories and files needed by IBM z/OS Platform for Apache Spark</title><description>This step will create the directories and files necessary for Apache Spark to write to.<p>IBM z/OS Platform for Apache Spark installs Apache Spark into a z/OS file system (zFS) or hierarchical file system (HFS) directory. 
This documentation refers to the installation directory as SPARK_HOME. The default installation directory is /usr/lpp/IBM/Spark.  
By default, Apache Spark runs from the installation directory, and most of its configuration files, log files, and working information are stored in the installation directory structure. 
On z/OS systems, however, the use of the installation directory for all of these purposes is not ideal operating behavior. 
Therefore, by default, IBM z/OS Platform for Apache Spark installs Apache Spark in a read-only file system.</p><p>In the steps below we describe how to set up customized directories for the Apache Spark configuration files, log files, and temporary work files.  
While you may customize the directory structure used by Apache Spark, the examples here follow the Filesystem Hierarchy Standard (FHS).
</p><p>Work with your system programmer who has authority to update system directories.  
</p><p><b>Note:</b> SPARK_HOME is an environment variable that is used by many Apache Spark scripts. 
This variable must contain the path to the IBM z/OS Platform for Apache Spark installation directory.</p></description><step name="Step3_4_1"><title>Create the configuration directory</title><description>The first of these new directories to be created is the configuration directory.  
In accordance with the Filesystem Hierarchy Standard (FHS), IBM recommends creating the new configuration directory under /etc. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions><ol>
<li>Open an OMVS or SSH shell environment and using the following command, create a new directory under /etc for the Apache Spark configuration files.  
For example, you might do the following:
<p><code>mkdir -p /etc/spark/conf</code></p>
</li>
<li>Provide read/write acess to the new directory to the user ID that runs IBM z/OS Platform for Apache Spark.</li>
<li>Ensure that the SPARK_CONF_DIR environment variable points to the new directory, for example:  
<code>export SPARK_CONF_DIR=/etc/spark/conf</code>
</li></ol></instructions><weight>1</weight></step><step name="Step3_4_2"><title>Copy and update the Apache Spark configuration files to the new configuration directory</title><description>Copy the Apache Spark configuration files to the new configuration directory and update them.  There are three main Apache Spark configuration files:
<ul><li><b>spark-env.sh</b> - A shell script that is sourced by most of the other scripts in the Apache Spark installation. 
You can use it to configure environment variables that set or alter the default values for various Apache Spark configuration settings.
</li>
<li><b>spark-defaults.conf</b> - A configuration file that sets default values for the Apache Spark runtime components. 
You can override these default values on the command line when you interact with Spark using shell scripts. 
</li>
<li><b>log4j.properties</b> - Contains the default configuration for log4j, the logging package that Apache Spark uses.  
</li></ul>Templates for these configuration files exist in the $SPARK_HOME/conf directory.</description>
<step name="Step3_4_2_1"><title>Copy the templates configuration files to your new configuration directory.</title><description>Copy the templates configuration files to your new configuration directory.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Copy the templates configuration files to your new configuration directory using the following commands:

<p><code>cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_CONF_DIR/spark-env.sh
cp $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_CONF_DIR/spark-	defaults.conf
cp $SPARK_HOME/conf/log4j.properties.template $SPARK_CONF_DIR/log4j.properties
</code></p></instructions><weight>1</weight></step><step name="Step3_4_2_2"><title>Update the configuration files as necessary  </title><description>Update the configuration files as necessary. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Update the configuration files as necessary.  For configuration samples, see  
<a href="https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azkic_r_sampconfigfiles.htm">Sample Apache Spark configuration files</a>
 in Appendix B in 
<cite>IBM z/OS Platform for Apache Spark Installation and Customization Guide</cite>.
<p>
<b>Note:</b> The spark-env.sh script must include environment variables pointing to the working directories created next.
This script must also be modified to export JAVA_HOME as noted in step 1.2.</p></instructions><weight>1</weight></step></step><step name="Step3_4_3"><title>Create the remaining working directories for Apache Spark </title><description>Create the remaining working directories for Apache Spark following your file system conventions.  </description><step name="Step3_4_3_1"><title>Decide where to create the Apache Spark working directories</title><description>Decide where to create the Apache Spark working directories. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description><instructions>The first table in the section on 
<a href="https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azkic_t_createworkdirs.htm">Creating the Apache Spark working directories</a> 
in <cite>IBM z/OS Platform for Apache Spark Installation and Customization Guide</cite> 
contains the working directory defaults.  
You can either take the defaults or create your own directories and configure Apache Spark to use the directories. <p>
For guidance on creating and mounting directories see <cite>z/OS UNIX System Services User's Guide</cite>.</p><p><b>Note:</b> Consider mounting the $SPARK_WORKER_DIR and $SPARK_LOCAL_DIRS on separate zFS file systems to avoid uncontrolled growth on the primary zFS where Spark is located. </p></instructions><weight>1</weight></step><step name="Step3_4_3_2"><title>Provide read/write access to the user ID that runs Spark </title><description>Provide read/write access to the user ID that runs Spark.  </description>
<instructions>For all new directories created, provide read/write access to the user ID that runs Apache Spark. 
</instructions><weight>1</weight></step><step name="Step3_4_3_3"><title>Update the $SPARK_CONF_DIR/spark-env.sh script with the new environment variables </title><description>Update the $SPARK_CONF_DIR/spark-env.sh script with the new environment variables pointing to the new working directories. <p>See the <b>Perform</b> workflow tab for further instructions.</p> </description>
<instructions>Update the $SPARK_CONF_DIR/spark-env.sh script with the new environment variables pointing to the new working directories.  
For example:
<p><code>export SPARK_WORKER_DIR=/var/spark/work
</code></p></instructions><weight>1</weight></step><step name="Step3_4_3_4"><title>Configure the working directories to be cleaned regularly </title><description>Configure the working directories to be cleaned regularly.</description><step name="Step3_4_3_4_1"><title>Configure Spark to perform cleanup</title><description>By default, Spark will not regularly clean up worker directories, but can be configured to do so.  
Change the following Spark properties in spark-defaults.conf to values which support your planned activity, and monitor these settings over time.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Monitor these settings over time:
<p><ol><li>spark.worker.cleanup.enabled - This enables periodic cleanup of workerand application directories. This is disabled by default.  Set to "true" to enable it.
</li>
<li>spark.worker.cleanup.interval - The frequency in seconds that the worker cleans up old application work directories.  The default is 30 minutes - decide whether this is adequate.
</li>
<li>spark.worker.cleanup.appDataTtl - Controls how long (in seconds) to retain application work directories. The default is 7 days, which is generally inadequate if Spark jobs 
run very frequently. Decide whether this is adequate.
</li></ol></p>
<p>For more information on these properties, see 
the following:</p><p><a href="http://spark.apache.org/docs/2.0.2/spark-standalone.html">http://spark.apache.org/docs/2.0.2/spark-standalone.html</a></p></instructions><weight>1</weight></step><step name="Step3_4_3_4_2"><title>Create jobs which will clean up or archive directories</title><description>Create jobs which will clean up or archive the working directories.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions><!--Yo Kristine, update this link after 4/14 drop!-->Create jobs which will clean up or archive the directories listed in "Creating the Apache Spark working directories" (<a href="https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azkic_t_createworkdirs.htm">https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azkic_t_createworkdirs.htm</a>)in 
<a href="https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/toc.htm"><cite>IBM z/OS Platform for Apache Spark Installation and Customization Guide</cite></a>:
<ol><li>$SPARK_LOG_DIR</li>
<li>$SPARK_WORKER_DIR (if not configured to be cleaned by Spark properties)</li>
<li>$SPARK_LOCAL_DIRS</li></ol><p>z/OS UNIX ships a sample script skulker which can be used as written, or modified to suit your specific needs, and 
regularly scheduled to run from cron or other in-house automation tooling. See 
<a href="https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.2.0/com.ibm.zos.v2r2.bpxa500/skulker.htm">skulker — Remove old files from a directory</a> 
and <a href="https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.2.0/com.ibm.zos.v2r2.bpxa500/crondmon.htm">cron daemon — Run commands at specified dates and times</a>
in  <cite>UNIX System Services Command Reference</cite> (<a>https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.2.0/com.ibm.zos.v2r2.bpxa500/toc.htm</a>).</p></instructions><weight>1</weight></step></step><step name="Step3_4_3_5"><title>Optionally, check all Spark file systems periodically</title><description>Optionally, check all Spark file systems periodically. For example, check $SPARK_HOME and any file systems mounted inside or elsewhere. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions><ol><li>You can associate the FSFULL BPXPRM<i>xx</i> parameter with a file system to generate operator messages as a file system reaches a user-specified threshold. See 
<cite><a href="https://www.ibm.com/support/knowledgecenter/SSLTBW_2.2.0/com.ibm.zos.v2r2.bpxb200/tfsfull.htm">Monitoring space in the TFS</a></cite> in 
<cite><a href="https://www.ibm.com/support/knowledgecenter/SSLTBW_2.2.0/com.ibm.zos.v2r2.bpxb200/toc.htm">z/OS UNIX System Services Planning</a></cite> 
(<a href="https://www.ibm.com/support/knowledgecenter/SSLTBW_2.2.0/com.ibm.zos.v2r2.bpxb200/toc.htm">https://www.ibm.com/support/knowledgecenter/SSLTBW_2.2.0/com.ibm.zos.v2r2.bpxb200/toc.htm</a>).</li>
<li>Look for the number of extents, which may impact performance of the packs involved.  If this needs addressed, create and mount a new zFS and use CopyTree, tar, or similiar utilities to copy the key directories from the old to the new.  
Then, unmount the old and mount the new in its place.</li> 
<li>See <cite><a href="https://www.ibm.com/support/knowledgecenter/SSLTBW_2.2.0/com.ibm.zos.v2r2.idas300/dasmov.htm">Managing File System Size</a></cite> in  <cite>z/OS DFSMSdfp Advanced Services</cite> 
(<a href="https://www.ibm.com/support/knowledgecenter/SSLTBW_2.2.0/com.ibm.zos.v2r2.idas300/toc.htm">https://www.ibm.com/support/knowledgecenter/SSLTBW_2.2.0/com.ibm.zos.v2r2.idas300/toc.htm</a>) for more information.</li></ol>
</instructions><weight>1</weight></step></step><step name="Step3_4_4"><title>Update the BPXPRMxx member with the information about the new Apache Spark file systems</title><description>Update the BPXPRM<i>xx</i> member with the information about the new Apache Spark file systems.</description>
<instructions>Update the BPXPRM<i>xx</i> member with the information about the new Apache Spark file systems.</instructions><weight>1</weight></step></step><step name="Step3_5"><title>Install and customize MDS</title><description>Go to 
<cite><a href="https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/dvs_ig_tsk_srvr_cnfg.htm">Customizing the Data Service server</a></cite> and 
<cite><a href="https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azk_ig_tsk_inst_studio.htm">Installing the Data Service Studio</a></cite>
<!-- the "IBM z/OS Platform for Apache Spark Data Service Server and Studio installation and customization" workflow--> in 
<a href="https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/toc.htm"><cite>IBM z/OS Platform for Apache Spark Installation and Customization Guide</cite></a> 
(<a href="https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/toc.htm">https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/toc.htm</a>) to install and customize 
MDS Data Service and Data Service Studio.</description>
<instructions>Go to the "IBM z/OS Platform for Apache Spark Data Service Server and Studio installation and customization" workflow or  <a href="https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/toc.htm"><cite>IBM z/OS Platform for Apache Spark Installation and Customization Guide</cite></a> 
(<a href="https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/toc.htm">https://www.ibm.com/support/knowledgecenter/en/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/toc.htm</a>) to install and customize 
MDS Data Service and Data Service Studio.</instructions><weight>1</weight></step></step><step name="Step4"><title>Configure networking</title><description>Configure networking</description><step name="Step4_1"><title>Configure your port settings</title><description>For your planned deployment and ecosystem, consider any port access and firewall implications 
and consider configuring specific port settings if needed.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description><instructions>For your planned deployment and ecosystem, consider any port access and firewall implications 
based on the ports below, and consider configuring specific port settings if needed.<p>Ports used by Spark on the cluster side include:
<table width="100%"><tr><th width="3*">Port name</th><th width="1*">Default port number</th><th width="3*">Configuration property</th><th width="3*">Notes</th></tr>
<tr><td>Master web UI</td><td>8080</td><td>spark.master.ui.port or SPARK_MASTER_WEBUI_PORT</td><td>The value set by the spark.master.ui.port property takes precedence.</td></tr>
<tr><td>Worker web UI</td><td>18080</td><td>spark.history.ui.port or SPARK_WORKER_WEBUI_PORT</td><td>Optional; only applies if you use the history server.</td></tr>
<tr><td>Master port</td><td>7077</td><td>SPARK_MASTER_PORT</td><td> </td></tr>
<tr><td>Master REST port</td><td>6066</td><td>spark.master.rest.port</td><td> </td></tr>
<tr><td>Worker port</td><td>(random)</td><td>SPARK_WORKER_PORT</td><td> </td></tr>
<tr><td>Executor port</td><td>(random)</td><td>spark.executor.port</td><td>For Spark 1.5.2 only. </td></tr>
<tr><td>Block manager port</td><td>(random)</td><td>spark.blockManager.port</td><td> </td></tr>
<tr><td>Shuffle server</td><td>7337</td><td>spark.shuffle.service.port</td><td>Optional; only applies if you use the external shuffle service. </td></tr>
</table>
</p><p>Ports used by Spark on the driver side include:
<table width="100%"><tr><th width="3*">Port name</th><th width="1*">Default port number</th><th width="3*">Configuration property</th><th width="3*">Notes</th></tr>
<tr><td>Application web UI</td><td>4040</td><td>spark.ui.port</td><td> </td></tr>
<tr><td>Driver port</td><td>(random)</td><td>spark.driver.port </td><td> </td></tr>
<tr><td>Block manager port</td><td>(random)</td><td>spark.blockManager.port</td><td> </td></tr>
<tr><td>File server</td><td>(random)</td><td>spark.fileserver.port</td><td>For Spark 1.5.2 only. </td></tr>
<tr><td>HTTP broadcast </td><td>(random)</td><td>spark.broadcast.port</td><td>For Spark 1.5.2 only. Not used if spark.broadcast.factory is set to
TorrentBroadcastFactory (default).</td></tr>
<tr><td>Class file server</td><td>(random)</td><td>spark.replClassServer.port</td><td>For Spark 1.5.2 only and only used in Spark shells.</td></tr>
</table>
</p><p>Every time a Spark process is started, a number of listening ports are created that are specific to that process’s intended function.  
Depending on your site policies, limit access to all ports, and specifically permit access for specific users or applications. </p><p>
In z/OS, you can enforce controls using Communications Server and RACF settings.</p><p>
Specifically, specifying PORT UNRSV DENY in your TCPIP.PROFILE will deny all applications access to unreserved ports for TCP or UDP.  
Additionally, PORT UNRSV SAF can be used to grant specific access to specific users, like the user ID starting the Spark cluster, and the users of Spark.  
See the PORT statement documentation in <cite><a href="https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.2.0/com.ibm.zos.v2r2.halz001/toc.htm">z/OS Communications Server: IP Configuration Reference</a></cite> 
(<a href="https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.2.0/com.ibm.zos.v2r2.halz001/toc.htm">https://www.ibm.com/support/knowledgecenter/en/SSLTBW_2.2.0/com.ibm.zos.v2r2.halz001/toc.htm</a>) for more information.
</p></instructions><weight>1</weight></step><step name="Step4_2"><title>Consider disabling the REST server</title><description>Consider disabling the REST server.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description><instructions>The REST server interface, listening on port 6066 by default, is currently not included in the 
Apache Spark documentation. However, Spark applications can be submitted through this interface.  The REST server does not support TLS nor client authentication. 
The REST server is used when applications are submitted using cluster deploy mode (--deploy-mode cluster).  Client deploy mode is the default behavior for Spark, and is 
how notebooks, like Jupyter Notebook, connect to a Spark cluster.  Depending on your planned deployment and environment, access to this REST server might be restricted by other controls.  
However, if you want to disable it, you can do so by setting <code>spark.master.rest.enabled</code> to <code>false</code>  in spark-defaults.conf.</instructions><weight>1</weight></step><step name="Step4_3"><title>Configure Spark environment variables for common Enterprise networking configurations.</title><description>Configure Spark environment variables for common Enterprise networking configurations.  Each of these can be set in spark-env.sh.</description><step name="Step4_3_1"><title>Set SPARK_PUBLIC_DNS to the external hostname to be used for the Spark Web UIs</title><description>Set SPARK_PUBLIC_DNS to the external hostname to be used for the Spark Web UIs.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description><instructions>For environments using Network Address Translation (NAT), set SPARK_PUBLIC_DNS to the external hostname to be used for the 
Spark Web UIs.  SPARK_PUBLIC_DNS sets the public DNS name of the Spark master and workers.  
This allows the Spark Master to present in the logs a URL with the hostname that is visible to the outside world.  </instructions><weight>1</weight></step><step name="Step4_3_2"><title>Set the SPARK_LOCAL_IP environment for listening ports</title><description>Set the SPARK_LOCAL_IP environment variable.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Set the SPARK_LOCAL_IP environment variable to configure Spark processes to bind to a specific and consistent IP address when creating listening ports. </instructions>
<weight>1</weight></step><step name="Step4_3_3"><title>Set the SPARK_MASTER_HOST environment variable properly</title><description>Set the SPARK_MASTER_HOST environment variable properly.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>On systems with multiple network adapters, Spark may attempt the default setting and give up if it doesn't work.  Set the SPARK_MASTER_HOST (prior to Spark 2.0 known as SPARK_MASTER_IP) 
environment variable to avoid this.</instructions><weight>1</weight></step></step></step><step name="Step5"><title>Configure IBM Java</title><description>Spark runs as several JVM processes.  The steps below help you to ensure that IBM Java is configured properly.  </description><step name="Step5_1"><title>Ensure correct Java configuration</title><description>Ensure the correct Java configuration for z/OS.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>See "Hints and Tips for Java on z/OS" at <a href="http://www.ibm.com/systems/z/os/zos/tools/java/faq/javafaq.html">http://www.ibm.com/systems/z/os/zos/tools/java/faq/javafaq.html</a> 
to ensure that your java configuration has appropriate settings for z/OS.  See also 
<a href="https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azkic_r_memcpuconfigopts.htm#azkic_r_memcpuconfigopts__tzosopts">Table 6</a> in 
<cite><a href="https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/topics/azkic_r_memcpuconfigopts.htm">Appendix C</a></cite> of the 
<cite><a href="https://www.ibm.com/support/knowledgecenter/SSCTFE_1.1.0/com.ibm.azk.v1r1.azka100/toc.htm">IBM z/OS Platform for Apache Spark Installation and Customization Guide</a></cite> for more settings.</instructions><weight>1</weight></step><step name="Step5_2"><title>Set environment setting  _CEE_DMPTARG</title><description>Set environment setting  _CEE_DMPTARG to store java dumps on a separate mount point, outside of $SPARK_HOME. <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Set environment setting  _CEE_DMPTARG to store java dumps on a separate mount point, outside of $SPARK_HOME.  For more information, see JVM environment settings in 
<cite>IBM® SDK, Java™ Technology Edition z/OS User Guide</cite> 
(at 
<a href="https://www.ibm.com/support/knowledgecenter/en/SSYKE2_8.0.0/com.ibm.java.zos.80.doc/diag/appendixes/env_var/env_jvm.html#env_jvm__dumps">https://www.ibm.com/support/knowledgecenter/en/SSYKE2_8.0.0/com.ibm.java.zos.80.doc/diag/appendixes/env_var/env_jvm.html#env_jvm__dumps</a>).
For information about the order that java takes dump settings, see "Dump agent environment variables" in 
<cite>IBM® SDK, Java™ Technology Edition Linux User Guide</cite> at 
<a href="https://www.ibm.com/support/knowledgecenter/SSYKE2_7.0.0/com.ibm.java.lnx.70.doc/diag/tools/dumpagents_env.html">https://www.ibm.com/support/knowledgecenter/SSYKE2_7.0.0/com.ibm.java.lnx.70.doc/diag/tools/dumpagents_env.html</a>
</instructions><weight>1</weight></step><step name="Step5_3"><title>Configure large page memory allocation for Java</title><description>Configure large page memory allocation for Java.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Configure large page memory allocation for Java. Configuration and best practices in setting max Java heap sizes are detailed in "Configuring large page memory allocation" in 
<cite>IBM® SDK, Java™ Technology Edition z/OS User Guide</cite> 
 (<a href="https://www.ibm.com/support/knowledgecenter/SSYKE2_8.0.0/com.ibm.java.zos.80.doc/user/alloc_large_page.html">https://www.ibm.com/support/knowledgecenter/SSYKE2_8.0.0/com.ibm.java.zos.80.doc/user/alloc_large_page.html</a>).
</instructions><weight>1</weight></step><step name="Step5_4"><title>Ensure zEDC is configured properly</title><description>If you have Java applications that will be using compression, ensure that zEDC is configured properly.  <p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>If you have Java applications that will be using compression, ensure zEDC is configured properly.  See "zEnterprise Data Compression" in <cite>IBM® SDK, Java™ Technology Edition z/OS User Guide</cite>
(<a href="https://www.ibm.com/support/knowledgecenter/en/SSYKE2_8.0.0/com.ibm.java.zos.80.doc/user/zedc_compression.html">https://www.ibm.com/support/knowledgecenter/en/SSYKE2_8.0.0/com.ibm.java.zos.80.doc/user/zedc_compression.html</a>) 
for more information.</instructions>
<weight>1</weight></step></step><step name="Step6"><title>Create jobs for starting and stopping Spark processes</title><description>Create jobs for starting and stopping Spark processes.</description><step name="Step6_1"><title>Start Spark processes</title><description>Spark processes, such as the master and worker, can be started through BPXBATCH.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>You can start Spark processes, such as the master and worker, using BPXPATCH. For example, here's sample of BPXBATCH logic to start the master and worker:
<p><code>//SPARKMST JOB 'SPARK START',CLASS=K,MSGCLASS=A,
// NOTIFY=&amp;SYSUID,SYSTEM=HOST,USER=SPARKID
//PRTDS EXEC PGM=BPXBATCH,REGION=0M
//STDPARM DD *
SH /bin/bash -l -c 'cd /usr/lpp/IBM/Spark/sbin;start-master.sh;sleep 10;
start-slave.sh spark://HOST.POK.IBM.COM:7077'
//SYSOUT DD SYSOUT=*
//STDIN DD DUMMY
//STDOUT DD SYSOUT=*
//STDERR DD SYSOUT=*
//
</code></p>
<p>And here's a sample of BPXBATCH logic to stop the master and worker:</p>
<p><code>//SPARKSTP JOB 'SPARK STOP',CLASS=K,MSGCLASS=A,
// NOTIFY=&amp;SYSUID,SYSTEM=HOST,USER=SPARKID
//PRTDS EXEC PGM=BPXBATCH,REGION=0M
//STDPARM DD *
SH /bin/bash -l -c 'cd /usr/lpp/IBM/Spark/sbin;stop-slave.sh;
sleep 10;stop-master.sh'
//SYSOUT DD SYSOUT=*
//STDIN DD DUMMY
//STDOUT DD SYSOUT=*
//STDERR DD SYSOUT=*
//
</code></p><p>These samples assume the user ID starting the Spark cluster is SPARKID, the default program (shell) of that user is bash, and that the Spark product is installed in /usr/lpp/IBM/Spark.  
The -l and -c options of bash instruct bash to run a login shell with the entire shell command sequence given between the single quotes.  
The semi-colons in the command sequence separate different shell commands. Specifically, the sample start job instructs bash to change directories to the Spark product install 
directory where admin commands are located:
<p><code>cd /usr/lpp/IBM/Spark/sbin</code></p>
</p><p>Then, it starts the master by issuing:
<p><code>start-master.sh</code></p>
<p>Next, it sleeps for 10 seconds to allow the master time to start:</p>
<p><code>sleep 10</code></p>
<p>Finally, it starts the worker:</p>
<p><code>start-slave.sh spark://HOST.POK.IBM.COM:7077</code></p> <p>where HOST.POK.IBM.COM is the name of the host where your master is listening, using port 7077 by default.  </p>
These start commands can also be run directly from z/OS UNIX instead of BPXBATCH as a quick test of your Spark configuration.</p><p>The sample job to stop Spark is similar, except it issues the specific Spark commands stop the worker,and then the master.</p><p>Consider testing the starting and stopping of the Spark cluster to ensure your setup is correct.  However, review the guidance in the  
<cite><a href="https://www-03.ibm.com/support/techdocs/atsmastr.nsf/WebIndex/WP102684">Resource Tuning Recommendations for IBM z/OS Platform for Apache Spark</a></cite> white paper (<code>https://www-03.ibm.com/support/techdocs/atsmastr.nsf/WebIndex/WP102684</code>) before going into production..</p>
</instructions><weight>1</weight></step></step><step name="Step7"><title>Tune Spark workloads</title><description>Tune Spark workloads.<p>See the <b>Perform</b> workflow tab for further instructions.</p></description>
<instructions>Go to the <cite><a href="https://www-03.ibm.com/support/techdocs/atsmastr.nsf/WebIndex/WP102684">Resource Tuning Recommendations for IBM z/OS Platform for Apache Spark </a></cite> 
white paper (<a href="https://www-03.ibm.com/support/techdocs/atsmastr.nsf/WebIndex/WP102684">https://www-03.ibm.com/support/techdocs/atsmastr.nsf/WebIndex/WP102684</a>)
<!--IBM z/OS Platform for Apache Spark Tuning Workflow workflow--> to assign resources (memory, processors) for Spark workloads.</instructions>
<weight>1</weight></step></workflow>
